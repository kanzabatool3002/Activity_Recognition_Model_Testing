{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mount the google Drive"
      ],
      "metadata": {
        "id": "2g0E5v5H2Qxl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMM2awxX_0k9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model for Action Recognition**\n",
        "\n",
        "This notebook demonstrates action recognition using a pre-trained Long-term Recurrent Convolutional Network (LRCN) or Conv-LSTM model. The model processes a sequence of video frames and predicts the action being performed.\n",
        "\n",
        "**Workflow:**\n",
        "\n",
        "\n",
        "\n",
        "*   Load the trained LRCN or Conv-LSTM model.\n",
        "*   Read the input video and extract frames.\n",
        "*   Pre-process frames by resizing and normalizing.\n",
        "*   Store frames in a sequence and pass them to the model.\n",
        "*   Predict the action for each sequence and overlay the result on the video frames.\n",
        "*   Save the output video with predicted labels displayed.\n",
        "*   The final output is a processed video where each frame is labeled with the predicted action, providing a real-time visual representation of activity recognition.\n"
      ],
      "metadata": {
        "id": "x3qJt9bZ3bSk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12q63VccKGt2"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from collections import deque\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# Set up model and sequence length\n",
        "LRCN_model = load_model(\"/content/LRCN_model___Date_Time_2024_10_13__04_01_18___Loss_0.38333097100257874___Accuracy_0.913165271282196.keras\")\n",
        "CLASSES_LIST = ['snatching', 'fighting', 'snatching']  # Modify this based on your classes\n",
        "SEQUENCE_LENGTH = 20  # Adjust according to your model input\n",
        "\n",
        "# Video dimensions\n",
        "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64  # Modify according to your model's input\n",
        "\n",
        "# Function to predict on already downloaded video\n",
        "def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):\n",
        "    '''\n",
        "    Perform action recognition on a video using the LRCN model.\n",
        "    Args:\n",
        "    video_file_path:  The path of the video on which action recognition is performed.\n",
        "    output_file_path: The path where the output video with the predicted action will be stored.\n",
        "    SEQUENCE_LENGTH:  The number of frames to pass to the model as one sequence.\n",
        "    '''\n",
        "\n",
        "    # Initialize the VideoCapture object to read from the video file.\n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        "\n",
        "    # Get the width and height of the video.\n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Initialize the VideoWriter Object to store the output video in the disk.\n",
        "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'),\n",
        "                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n",
        "\n",
        "    # Declare a queue to store video frames.\n",
        "    frames_queue = deque(maxlen=SEQUENCE_LENGTH)\n",
        "\n",
        "    # Initialize a variable to store the predicted action being performed in the video.\n",
        "    predicted_class_name = ''\n",
        "\n",
        "    # Iterate until the video is accessed successfully.\n",
        "    while video_reader.isOpened():\n",
        "        # Read the frame.\n",
        "        ok, frame = video_reader.read()\n",
        "\n",
        "        # Check if the frame is not read properly, then break the loop.\n",
        "        if not ok:\n",
        "            break\n",
        "\n",
        "        # Resize and normalize the frame.\n",
        "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "        normalized_frame = resized_frame / 255.0\n",
        "\n",
        "        # Append the pre-processed frame into the frames list.\n",
        "        frames_queue.append(normalized_frame)\n",
        "\n",
        "        # Check if the number of frames in the queue equals the sequence length.\n",
        "        if len(frames_queue) == SEQUENCE_LENGTH:\n",
        "            # Pass the normalized frames to the model and get the predicted probabilities.\n",
        "            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis=0))[0]\n",
        "\n",
        "            # Get the index of class with highest probability.\n",
        "            predicted_label = np.argmax(predicted_labels_probabilities)\n",
        "\n",
        "            # Get the class name using the retrieved index.\n",
        "            predicted_class_name = CLASSES_LIST[predicted_label]\n",
        "\n",
        "        # Write predicted class name on top of the frame.\n",
        "        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Write the frame into the disk using the VideoWriter Object.\n",
        "        video_writer.write(frame)\n",
        "\n",
        "    # Release the VideoCapture and VideoWriter objects.\n",
        "    video_reader.release()\n",
        "    video_writer.release()\n",
        "\n",
        "# Path to your already downloaded video\n",
        "input_video_file_path = '/content/Mobile_snatching_1.mp4'  # Change this to the path of your downloaded video\n",
        "# this code is not performing well, it only predicts the 1st class of list\n",
        "# Construct the output video path\n",
        "output_video_file_path = '/content/-Output-SeqLen.mp4'\n",
        "\n",
        "# Perform Action Recognition on the Test Video\n",
        "predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)\n",
        "\n",
        "# Display the output video\n",
        "VideoFileClip(output_video_file_path, audio=False, target_resolution=(300, None)).ipython_display()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GZ1uh7uyRUn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import load_model\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# Define the path to the pre-trained models\n",
        "LRCN_MODEL_PATH = '/content/LRCN_model___Date_Time_2024_10_13__04_01_18___Loss_0.38333097100257874___Accuracy_0.913165271282196.keras'\n",
        "CONVLSTM_MODEL_PATH = '/content/convlstm_model___Date_Time_2024_10_12__20_09_35___Loss_0.6140506267547607___Accuracy_0.8123249411582947.keras'\n",
        "\n",
        "# Load the models\n",
        "LRCN_model = load_model(LRCN_MODEL_PATH)\n",
        "convlstm_model = load_model(CONVLSTM_MODEL_PATH)\n",
        "\n",
        "# Set the sequence length and image dimensions\n",
        "SEQUENCE_LENGTH = 20\n",
        "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64  # Adjust these based on your model's input size\n",
        "CLASSES_LIST = [\"Fighting\", \"Normal\", \"Snatching\"]  # Update based on your classes\n",
        "\n",
        "# Function to perform action recognition on a video and draw bounding boxes\n",
        "def analyze_and_predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH, frame_skip=5):\n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n",
        "\n",
        "    frames_queue = deque(maxlen=SEQUENCE_LENGTH)\n",
        "    frames_list = []\n",
        "    action_predictions = []\n",
        "\n",
        "    # Read the video frames\n",
        "    while video_reader.isOpened():\n",
        "        ok, frame = video_reader.read()\n",
        "        if not ok:\n",
        "            break\n",
        "\n",
        "        frames_list.append(frame)\n",
        "        if len(frames_list) >= SEQUENCE_LENGTH:\n",
        "            # Prepare the frames for prediction\n",
        "            processed_frames = []\n",
        "            for frm in frames_list[-SEQUENCE_LENGTH:]:\n",
        "                resized_frame = cv2.resize(frm, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "                normalized_frame = resized_frame / 255.0\n",
        "                processed_frames.append(normalized_frame)\n",
        "\n",
        "            # Perform action prediction\n",
        "            predicted_labels_probabilities = convlstm_model.predict(np.expand_dims(processed_frames, axis=0))[0]\n",
        "\n",
        "            # Print raw predicted probabilities for each class\n",
        "            print(f\"Predicted probabilities: {predicted_labels_probabilities}\")\n",
        "\n",
        "            # Get the index of the class with the highest probability\n",
        "            predicted_label = np.argmax(predicted_labels_probabilities)\n",
        "\n",
        "            # Get the class name using the retrieved index\n",
        "            predicted_class_name = CLASSES_LIST[predicted_label]\n",
        "            action_predictions.append(predicted_class_name)\n",
        "        else:\n",
        "            action_predictions.append(\"\")\n",
        "\n",
        "    video_reader.release()\n",
        "\n",
        "    # Annotate the video with predictions\n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n",
        "\n",
        "    frame_counter = 0\n",
        "    while video_reader.isOpened():\n",
        "        ok, frame = video_reader.read()\n",
        "        if not ok:\n",
        "            break\n",
        "\n",
        "        # Draw bounding boxes or text on the frame\n",
        "        if frame_counter >= SEQUENCE_LENGTH:\n",
        "            predicted_class_name = action_predictions[frame_counter - SEQUENCE_LENGTH]\n",
        "            if predicted_class_name:\n",
        "                # Example box; adjust as needed\n",
        "                cv2.rectangle(frame, (10, 10), (200, 60), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, predicted_class_name, (15, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        video_writer.write(frame)\n",
        "        frame_counter += 1\n",
        "\n",
        "    video_reader.release()\n",
        "    video_writer.release()\n",
        "\n",
        "# Upload and process the video\n",
        "uploaded_video_file_path = '/content/Mobile_snatching_1.mp4'  # Replace this with the actual path of the uploaded video\n",
        "\n",
        "# Construct the output video path\n",
        "output_video_file_path = f'{os.path.splitext(uploaded_video_file_path)[0]}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'\n",
        "\n",
        "# Perform action recognition and draw bounding boxes\n",
        "analyze_and_predict_on_video(uploaded_video_file_path, output_video_file_path, SEQUENCE_LENGTH)\n",
        "\n",
        "# Display the output video\n",
        "VideoFileClip(output_video_file_path, audio=False, target_resolution=(300, None)).ipython_display()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMNyxTqQ8LHG"
      },
      "source": [
        "Single Frame Prediction on videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JqtUUjvZ8JZW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# Define the path to the pre-trained models\n",
        "LRCN_MODEL_PATH = '/content/LRCN_model___Date_Time_2024_10_13__04_01_18___Loss_0.38333097100257874___Accuracy_0.913165271282196.keras'\n",
        "CONVLSTM_MODEL_PATH = '/content/improve_convlstm_model___Date_Time_2024_10_12__20_42_24___Loss_0.41087716817855835___Accuracy_0.8095238208770752.keras'\n",
        "\n",
        "# Load the models\n",
        "LRCN_model = load_model(LRCN_MODEL_PATH)\n",
        "convlstm_model = load_model(CONVLSTM_MODEL_PATH)\n",
        "\n",
        "# Set the sequence length and image dimensions\n",
        "SEQUENCE_LENGTH = 20\n",
        "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64  # Adjust these based on your model's input size\n",
        "CLASSES_LIST = [\"Fighting\", \"Snatching\"]  # Update based on your classes\n",
        "\n",
        "# Function to perform action recognition on a video and draw bounding boxes\n",
        "# def analyze_and_predict_on_video(video_file_path, output_file_path):\n",
        "def analyze_and_predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):\n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n",
        "\n",
        "    frames_queue = []  # Use a list to maintain the latest frames for prediction\n",
        "    action_predictions = []\n",
        "\n",
        "    # Set a confidence threshold\n",
        "    CONFIDENCE_THRESHOLD = 0.5\n",
        "\n",
        "    # Read the video frames\n",
        "    while video_reader.isOpened():\n",
        "        ok, frame = video_reader.read()\n",
        "        if not ok:\n",
        "            break\n",
        "\n",
        "        # Resize and normalize the frame\n",
        "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "        normalized_frame = resized_frame / 255.0\n",
        "\n",
        "        # Append the processed frame to the queue\n",
        "        frames_queue.append(normalized_frame)\n",
        "\n",
        "        # If we have enough frames, make a prediction\n",
        "        if len(frames_queue) >= SEQUENCE_LENGTH:\n",
        "            # Prepare the input for the model (5D tensor)\n",
        "            input_sequence = np.array(frames_queue[-SEQUENCE_LENGTH:])\n",
        "            input_sequence = np.expand_dims(input_sequence, axis=0)\n",
        "\n",
        "            # Make the prediction\n",
        "            predicted_labels_probabilities = convlstm_model.predict(input_sequence)[0]\n",
        "\n",
        "            # Print raw predicted probabilities for each class\n",
        "            print(f\"Predicted probabilities for frame: {predicted_labels_probabilities}\")\n",
        "\n",
        "            # Annotate the frame based on confidence threshold\n",
        "            if np.max(predicted_labels_probabilities) >= CONFIDENCE_THRESHOLD:\n",
        "                predicted_label = np.argmax(predicted_labels_probabilities)\n",
        "                predicted_class_name = CLASSES_LIST[predicted_label]\n",
        "                action_predictions.append(predicted_class_name)\n",
        "                cv2.putText(frame, predicted_class_name, (15, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "            else:\n",
        "                action_predictions.append(\"Uncertain\")\n",
        "                cv2.putText(frame, \"Uncertain\", (15, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "        else:\n",
        "            action_predictions.append(\"\")  # Not enough frames to predict yet\n",
        "\n",
        "        # Write the annotated frame to the output video\n",
        "        video_writer.write(frame)\n",
        "\n",
        "    video_reader.release()\n",
        "    video_writer.release()\n",
        "\n",
        "\n",
        "# Upload and process the video\n",
        "uploaded_video_file_path = '/content/fighting.mp4'  # Replace this with the actual path of the uploaded video\n",
        "\n",
        "# Construct the output video path\n",
        "output_video_file_path = f'{os.path.splitext(uploaded_video_file_path)[0]}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'\n",
        "\n",
        "# Perform action recognition and draw bounding boxes\n",
        "analyze_and_predict_on_video(uploaded_video_file_path, output_video_file_path, SEQUENCE_LENGTH)\n",
        "\n",
        "# Display the output video\n",
        "VideoFileClip(output_video_file_path, audio=False, target_resolution=(300, None)).ipython_display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WljrHr2S8vx1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}